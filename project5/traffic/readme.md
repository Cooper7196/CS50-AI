I experimented with many different layers in my neural network. I tried to use different activation functions, different loss functions, and different optimizers. I found that using a few convolution layers with increasing number of filters in each layer. I found that the ReLU activation layer, and the Adam optimizer worked best. This was true for the intermidiary layers, but I foudn sigmoid worked better for the final output. I also found that I could lower the dropout rate to as low as 0.2 without any significant overfitting. This caused a great improvement to the accuracy rate which I managed to get to 98 to 99 percent with the testing dataset.